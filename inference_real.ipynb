{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install all dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the dependencies using the lsi.yml file provided in the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from options.train_options import TrainOptions\n",
    "from models.pSp_o import pSp_o\n",
    "import torchvision.transforms as trans\n",
    "import glob\n",
    "import facer\n",
    "import os\n",
    "from utils.metrics_downstream import plot_land_overlay, plot_segmentation_overlay, Metrics, load_attributes\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Image transforms function\n",
    "image_transforms = trans.Compose([\n",
    "        trans.Resize((256, 256)),\n",
    "\t\ttrans.ToTensor(),\n",
    "\t\ttrans.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "\t])\n",
    "\n",
    "#Retrieve default args\n",
    "opts = TrainOptions()\n",
    "opts, unknown = opts.parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the pre-trained models and checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models and data can be downloaded at: (https://drive.google.com/drive/folders/11g9t4ZLKhMfigPRa7dLdqmp5vlThFgoB?usp=drive_link)\n",
    "\n",
    "\n",
    "Place them inside their respective folders in the main directory.\n",
    "- ./paper_checkpoints\n",
    "    - Contains the checkpoints for the paper models\n",
    "- ./pretrained_models\n",
    "    - Pre-trained models used in loss functions and the generative models themselves\n",
    "- ./datasets/images and datasets/real/\n",
    "    - Input images and experimental measurements used on paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compression Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real will load the physically measured intermediate latents.\n",
    "opts.ini_w = \"real\"\n",
    "\n",
    "compression_dict = {\n",
    "    \"1:512\": 128, # 1x128 measurements\n",
    "    \"1:2048\": 32, # 1x32 measurements\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_scalars = compression_dict[\"1:512\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Models and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts.device = 'cuda:0'\n",
    "opts.cls = True\n",
    "opts.segment = True\n",
    "opts.landmarks = True\n",
    "opts.segment_fts = True \n",
    "opts.encoding_size =  n_scalars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_scalars == 128:\n",
    "    opts.checkpoint_path = \"paper_checkpoints/real_128.pt\"\n",
    "    path = \"datasets/real/128/\"\n",
    "elif n_scalars == 32:\n",
    "    opts.checkpoint_path = \"paper_checkpoints/real_32.pt\"\n",
    "    path = \"datasets/real/32/\"\n",
    "else:\n",
    "    raise ValueError(f\"Invalid compression ratio: {n_scalars}\")\n",
    "images = sorted(glob.glob(path+\"*.npz\"))\n",
    "print(\"Number of images:\",len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Loading model components ===\")\n",
    "net = pSp_o(opts).to(opts.device)\n",
    "net = net.to(opts.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = f\"out/real/{n_scalars}/\"\n",
    "\n",
    "if os.path.exists(path_out):\n",
    "\t\tpass\n",
    "else:\n",
    "    os.makedirs(path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for i,image in enumerate(images):\n",
    "        name = (image.split(\"/\")[-1])\n",
    "\n",
    "        # Load read measurement\n",
    "        image = np.load(image)\n",
    "        image = torch.from_numpy(image['array']).to(opts.device)\n",
    "        out, test, latent_w_plus = net.forward(image.unsqueeze(0),return_latents=True, global_step=0)\n",
    "    \n",
    "        # Load GT image\n",
    "        image = Image.open(\"datasets/images/\"+name[:-4]+\".jpg\")\n",
    "        image = image_transforms(image).unsqueeze(0).to(opts.device)\n",
    "        \n",
    "        ## Landmarks Evaluation\n",
    "        plot_land_overlay(torch.zeros_like(out)-1, test[2][1], name=path_out+name[:-4]+\"_land.png\",new_logits=None,alpha=0.2)\n",
    "        \n",
    "        #Segmentation Evaluation\n",
    "        plot_segmentation_overlay(torch.zeros_like(image)-1, test[2][0].detach().to(\"cpu\"), name=path_out+name[:-4]+\"_seg.png\", logits_f=True, alpha=1.0)\n",
    "        \n",
    "        #Reconstruction\n",
    "        out = (((out*0.5)+0.5)*255).clamp(0,255)\n",
    "        out = out.squeeze(0).to(\"cpu\").permute(1,2,0).detach().numpy().astype(np.uint8)\n",
    "        Image.fromarray(out).save(path_out+name[:-4]+\"_rec.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading ground truth attributes and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_gt = load_attributes(\"datasets/list_attr_celeba.txt\", images)\n",
    "metrics = Metrics(19, ['background', 'neck', 'face', 'cloth', 'rr', 'lr', 'rb', 'lb', 're', 'le', 'nose', 'imouth', 'llip', 'ulip', 'hair', 'eyeg', 'hat', 'earr', 'neck_l'])\n",
    "metrics.reset()\n",
    "\n",
    "face_detector = facer.face_detector('retinaface/mobilenet', device=\"cuda:0\")\n",
    "face_landmark = facer.face_aligner('farl/ibug300w/448', device=\"cuda:0\")\n",
    "face_parser = facer.face_parser('farl/celebm/448', device=\"cuda:0\")\n",
    "face_attr = facer.face_attr(\"farl/celeba/224\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "with open(path_out+\"output.txt\", \"w\") as file:\n",
    "    file.write(\"Name, D, L2, NME \\n\")\n",
    "    d1 = 0\n",
    "    nme1 = 0\n",
    "    l21 = 0\n",
    "    total = 0\n",
    "    accuracy_t = 0\n",
    "    with torch.no_grad():\n",
    "        for i,image in enumerate(images):\n",
    "            name = (image.split(\"/\")[-1])\n",
    "\n",
    "            # Load read measurement\n",
    "            image = np.load(image)\n",
    "            image = torch.from_numpy(image['array']).to(opts.device)\n",
    "            out, test, latent_w_plus = net.forward(image.unsqueeze(0),return_latents=True, global_step=0)\n",
    "        \n",
    "            # Load GT image\n",
    "            image = Image.open(\"datasets/images/\"+name[:-4]+\".jpg\")\n",
    "            image = image_transforms(image).unsqueeze(0).to(opts.device)\n",
    "            \n",
    "            ## Landmarks Evaluation\n",
    "            input_gt = ((image.detach() + 1) / 2 * 255).clamp(0, 255).to(torch.uint8)\n",
    "            faces = face_detector(input_gt)\n",
    "            gt = ((face_landmark(input_gt,faces)['alignment'])[0])#/255.0\n",
    "            x_min, _ = torch.min(gt[:, 0], dim=0)\n",
    "            y_min, _ = torch.min(gt[:, 1], dim=0)\n",
    "            x_max, _ = torch.max(gt[:, 0], dim=0)\n",
    "            y_max, _ = torch.max(gt[:, 1], dim=0)\n",
    "            # Calculate the normalization factor d (bounding box diagonal)\n",
    "            d = torch.sqrt((x_max - x_min) ** 2 + (y_max - y_min) ** 2)\n",
    "\n",
    "            # Calculate the Euclidean distances between corresponding landmarks\n",
    "            distances = torch.sqrt(torch.sum((test[2][1][0]*255 - gt) ** 2, dim=1))\n",
    "            l2 = distances.mean()\n",
    "            nme = ((distances/d).mean())*100 #100 is arbitrary, just for visualization purposes.\n",
    "            d1 += d\n",
    "            nme1 += nme\n",
    "            l21 += l2\n",
    "            total+=1\n",
    "            \n",
    "            #Segmentation Evaluation\n",
    "            gt_seg = ((face_parser(input_gt,faces)['seg']['logits']))\n",
    "            gt_seg = gt_seg.argmax(dim=1).float()\n",
    "            y_pred = torch.argmax(test[2][0], dim=1) # get the most likely prediction\n",
    "            metrics.add_batch(gt_seg.detach().cpu().numpy(), y_pred.detach().cpu().numpy())\n",
    "\n",
    "            #Attributes Evaluation\n",
    "            att = (attributes_gt[i]+1)/2\n",
    "            correct_predictions = torch.sum(torch.round(F.sigmoid(test[2][2])).to(\"cpu\") == att,1)\n",
    "            accuracy = ((correct_predictions / 40) * 100)\n",
    "            accuracy_t += accuracy\n",
    " \n",
    "        file.write(\"{}, {}, {}, {}\\n\".format(\"Landmarks Metrics:\", d1/total, l21/total, nme1/total))  \n",
    "        file.write(\"{}{}\\n\".format(\"Segmentation Metrics:\\n\",metrics.get_table()))  \n",
    "        file.write(\"{}, {}\\n\".format(\"Accuracy Classification:\",(accuracy_t/total).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
